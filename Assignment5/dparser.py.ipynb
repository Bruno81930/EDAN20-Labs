{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n",
      "1000 sentences on 11042\n",
      "2000 sentences on 11042\n",
      "3000 sentences on 11042\n",
      "4000 sentences on 11042\n",
      "5000 sentences on 11042\n",
      "6000 sentences on 11042\n",
      "7000 sentences on 11042\n",
      "8000 sentences on 11042\n",
      "9000 sentences on 11042\n",
      "10000 sentences on 11042\n",
      "11000 sentences on 11042\n",
      "x = [ stack0_POS: nil, stack1_POS: nil, stack0_word: nil, stack1_word: nil, queue0_POS: ROOT, queue1_POS: NN, queue0_word: ROOT, queue1_word: Äktenskapet, stack0fw_POS: nil, stack0fw_word: nil, stack1h_POS: nil, stack1h_word: nil, can-re: False, can-la: False ], y = sh\n",
      "x = [ stack0_POS: ROOT, stack1_POS: nil, stack0_word: ROOT, stack1_word: nil, queue0_POS: NN, queue1_POS: ++, queue0_word: Äktenskapet, queue1_word: och, stack0fw_POS: NN, stack0fw_word: Äktenskapet, stack1h_POS: nil, stack1h_word: nil, can-re: True, can-la: False ], y = sh\n",
      "x = [ stack0_POS: NN, stack1_POS: ROOT, stack0_word: Äktenskapet, stack1_word: ROOT, queue0_POS: ++, queue1_POS: NN, queue0_word: och, queue1_word: familjen, stack0fw_POS: ++, stack0fw_word: och, stack1h_POS: ROOT, stack1h_word: ROOT, can-re: False, can-la: True ], y = sh\n",
      "x = [ stack0_POS: ++, stack1_POS: NN, stack0_word: och, stack1_word: Äktenskapet, queue0_POS: NN, queue1_POS: AV, queue0_word: familjen, queue1_word: är, stack0fw_POS: NN, stack0fw_word: familjen, stack1h_POS: AV, stack1h_word: är, can-re: False, can-la: True ], y = la.++\n",
      "x = [ stack0_POS: NN, stack1_POS: ROOT, stack0_word: Äktenskapet, stack1_word: ROOT, queue0_POS: NN, queue1_POS: AV, queue0_word: familjen, queue1_word: är, stack0fw_POS: ++, stack0fw_word: och, stack1h_POS: ROOT, stack1h_word: ROOT, can-re: False, can-la: True ], y = ra.CC\n",
      "x = [ stack0_POS: NN, stack1_POS: NN, stack0_word: familjen, stack1_word: Äktenskapet, queue0_POS: AV, queue1_POS: EN, queue0_word: är, queue1_word: en, stack0fw_POS: AV, stack0fw_word: är, stack1h_POS: AV, stack1h_word: är, can-re: True, can-la: False ], y = re\n",
      "x = [ stack0_POS: NN, stack1_POS: ROOT, stack0_word: Äktenskapet, stack1_word: ROOT, queue0_POS: AV, queue1_POS: EN, queue0_word: är, queue1_word: en, stack0fw_POS: ++, stack0fw_word: och, stack1h_POS: ROOT, stack1h_word: ROOT, can-re: False, can-la: True ], y = la.SS\n",
      "x = [ stack0_POS: ROOT, stack1_POS: nil, stack0_word: ROOT, stack1_word: nil, queue0_POS: AV, queue1_POS: EN, queue0_word: är, queue1_word: en, stack0fw_POS: NN, stack0fw_word: Äktenskapet, stack1h_POS: nil, stack1h_word: nil, can-re: True, can-la: False ], y = ra.ROOT\n",
      "x = [ stack0_POS: AV, stack1_POS: ROOT, stack0_word: är, stack1_word: ROOT, queue0_POS: EN, queue1_POS: AJ, queue0_word: en, queue1_word: gammal, stack0fw_POS: EN, stack0fw_word: en, stack1h_POS: ROOT, stack1h_word: ROOT, can-re: True, can-la: False ], y = sh\n",
      "Encoding the features and classes...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import transition\n",
    "import conll\n",
    "import features\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "def reference(stack, queue, graph):\n",
    "    \"\"\"\n",
    "    Gold standard parsing\n",
    "    Produces a sequence of transitions from a manually-annotated corpus:\n",
    "    sh, re, ra.deprel, la.deprel\n",
    "    :param stack: The stack\n",
    "    :param queue: The input list\n",
    "    :param graph: The set of relations already parsed\n",
    "    :return: the transition and the grammatical function (deprel) in the\n",
    "    form of transition.deprel\n",
    "    \"\"\"\n",
    "    # Right arc\n",
    "    if stack and stack[0]['id'] == queue[0]['head']:\n",
    "        #print('ra', queue[0]['deprel'], stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "        deprel = '.' + queue[0]['deprel']\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'ra'+ deprel\n",
    "    # Left arc\n",
    "    if stack and queue[0]['id'] == stack[0]['head']:\n",
    "        #print('la', stack[0]['deprel'], stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "        deprel = '.' + stack[0]['deprel']\n",
    "        stack, queue, graph = transition.left_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'la' + deprel\n",
    "    # Reduce\n",
    "    if stack and transition.can_reduce(stack, graph):\n",
    "        for word in stack:\n",
    "            if (word['id'] == queue[0]['head'] or\n",
    "                        word['head'] == queue[0]['id']):\n",
    "                # print('re', stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "                stack, queue, graph = transition.reduce(stack, queue, graph)\n",
    "                return stack, queue, graph, 're'\n",
    "    # Shift\n",
    "    # print('sh', [], queue[0]['cpostag'])\n",
    "    stack, queue, graph = transition.shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'\n",
    "\n",
    "def extract(stack, queue, graph, feature_names, sentence):\n",
    "    X = []\n",
    "    X.append(transition.can_leftarc(stack, graph))\n",
    "    X.append(transition.can_reduce(stack, graph))\n",
    "    try:\n",
    "        X.append(stack[0]['postag'])\n",
    "        X.append(stack[0]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    try:\n",
    "        X.append(stack[1]['postag'])\n",
    "        X.append(stack[1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X1 = X\n",
    "    try:\n",
    "        X.append(queue[0]['postag'])\n",
    "        X.append(queue[0]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    try:\n",
    "        X.append(queue[1]['postag'])\n",
    "        X.append(queue[1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X2 = X\n",
    "    try:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i]['form'] == stack[0]['form']:\n",
    "                X.append(sentence[i+1]['postag'])\n",
    "                X.append(sentence[i+1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "        \n",
    "    try:\n",
    "        X.append(sentence[int(stack[1]['head'])]['postag'])\n",
    "        X.append(sentence[int(stack[1]['head'])]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X3 = X\n",
    "    X1 = dict(zip(feature_names[:6], X1))\n",
    "    X2 = dict(zip(feature_names[:10], X2))\n",
    "    X3 = dict(zip(feature_names, X3))\n",
    "    return X1, X2, X3\n",
    "\n",
    "def encode_classes(y_symbols):\n",
    "    \n",
    "    # We extract the chunk names\n",
    "    classes = sorted(list(set(y_symbols)))\n",
    "    \n",
    "    # We assign each name a number\n",
    "    dict_classes = dict(enumerate(classes))\n",
    "\n",
    "    # We build an inverted dictionary\n",
    "    inv_dict_classes = {v: k for k, v in dict_classes.items()}\n",
    "\n",
    "    # We convert y_symbols into a numerical vector\n",
    "    y = [inv_dict_classes[i] for i in y_symbols]\n",
    "    \n",
    "    return y, dict_classes, inv_dict_classes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_file = './swedish_talbanken05_train.conll'\n",
    "    test_file = './swedish_talbanken05_test_blind.conll'\n",
    "    column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "    column_names_2006_test = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats']\n",
    "    feature_names = ['can-la', 'can-re','stack0_POS', 'stack0_word', 'stack1_POS', 'stack1_word', 'queue0_POS', 'queue0_word', 'queue1_POS', 'queue1_word', 'stack0fw_POS', 'stack0fw_word','stack1h_POS','stack1h_word']\n",
    "\n",
    "    sentences = conll.read_sentences(train_file)\n",
    "    formatted_corpus = conll.split_rows(sentences, column_names_2006)\n",
    "    \n",
    "    sent_cnt = 0\n",
    "    X1_dict = []\n",
    "    X2_dict = []\n",
    "    X3_dict = []\n",
    "    Y = []\n",
    "    print(\"Extracting the features...\")\n",
    "    for sentence in formatted_corpus:\n",
    "        sent_cnt += 1\n",
    "        if sent_cnt % 1000 == 0:\n",
    "            print(sent_cnt, 'sentences on', len(formatted_corpus), flush=True)\n",
    "        stack = []\n",
    "        queue = list(sentence)\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        while queue:\n",
    "            x1, x2, x3 = extract(stack, queue, graph, feature_names, sentence)\n",
    "            X1_dict.append(x1)\n",
    "            X2_dict.append(x2)\n",
    "            X3_dict.append(x3)\n",
    "            stack, queue, graph, trans = reference(stack, queue, graph)\n",
    "            Y.append(trans)\n",
    "        stack, graph = transition.empty_stack(stack, graph)\n",
    "        #print('Equal graphs:', transition.equal_graphs(sentence, graph))\n",
    "\n",
    "        # Poorman's projectivization to have well-formed graphs.\n",
    "        for word in sentence:\n",
    "            word['head'] = graph['heads'][word['id']]\n",
    "        \n",
    "    print(\"Encoding the features and classes...\")\n",
    "    # Vectorize the feature matrix and carry out a one-hot encoding\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    X1 = vec.fit_transform(X1_dict)\n",
    "    X2 = vec.fit_transform(X2_dict)\n",
    "    X3 = vec.fit_transform(X3_dict)\n",
    "    y, dict_classes, inv_dict_classes = encode_classes(Y)\n",
    "    \n",
    "    print(\"Training the models...\")\n",
    "    classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "    print(\"Training the first model...\")\n",
    "    model1_train_time = time.clock() \n",
    "    model1 = classifier.fit(X1, y)\n",
    "    print(\"Model 1 training time = \" + str((time.clock() - model1_train_time)/60))\n",
    "    print(\"Training the second model...\")\n",
    "    model2_train_time = time.clock()\n",
    "    model2 = classifier.fit(X2, y)\n",
    "    print(\"Model 2 training time = \" + str((time.clock() - model2_train_time)/60))\n",
    "    print(\"Training the third model...\")\n",
    "    model3_train_time = time.clock()\n",
    "    model3 = classifier.fit(X3, y)\n",
    "    print(\"Model 3 training time = \" + str((time.clock() - model2_train_time)/60))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
