{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n",
      "Encoding the features and classes...\n",
      "Training the models...\n",
      "Training the first model...\n",
      "Model 1 training time = 9.999999974752428e-08\n",
      "Training the second model...\n",
      "Model 2 training time = 5.0000001768542766e-08\n",
      "Training the third model...\n",
      "Model 3 training time = 8.764225033333332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import transition\n",
    "import conll\n",
    "import features\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import linear_model\n",
    "\n",
    "def reference(stack, queue, graph):\n",
    "    \"\"\"\n",
    "    Gold standard parsing\n",
    "    Produces a sequence of transitions from a manually-annotated corpus:\n",
    "    sh, re, ra.deprel, la.deprel\n",
    "    :param stack: The stack\n",
    "    :param queue: The input list\n",
    "    :param graph: The set of relations already parsed\n",
    "    :return: the transition and the grammatical function (deprel) in the\n",
    "    form of transition.deprel\n",
    "    \"\"\"\n",
    "    # Right arc\n",
    "    if stack and stack[0]['id'] == queue[0]['head']:\n",
    "        #print('ra', queue[0]['deprel'], stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "        deprel = '.' + queue[0]['deprel']\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'ra'+ deprel\n",
    "    # Left arc\n",
    "    if stack and queue[0]['id'] == stack[0]['head']:\n",
    "        #print('la', stack[0]['deprel'], stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "        deprel = '.' + stack[0]['deprel']\n",
    "        stack, queue, graph = transition.left_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'la' + deprel\n",
    "    # Reduce\n",
    "    if stack and transition.can_reduce(stack, graph):\n",
    "        for word in stack:\n",
    "            if (word['id'] == queue[0]['head'] or\n",
    "                        word['head'] == queue[0]['id']):\n",
    "                # print('re', stack[0]['cpostag'], queue[0]['cpostag'])\n",
    "                stack, queue, graph = transition.reduce(stack, queue, graph)\n",
    "                return stack, queue, graph, 're'\n",
    "    # Shift\n",
    "    # print('sh', [], queue[0]['cpostag'])\n",
    "    stack, queue, graph = transition.shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'\n",
    "\n",
    "def extract(stack, queue, graph, feature_names, sentence):\n",
    "    X = []\n",
    "    X.append(transition.can_leftarc(stack, graph))\n",
    "    X.append(transition.can_reduce(stack, graph))\n",
    "    try:\n",
    "        X.append(stack[0]['postag'])\n",
    "        X.append(stack[0]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    try:\n",
    "        X.append(stack[1]['postag'])\n",
    "        X.append(stack[1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X1 = X\n",
    "    try:\n",
    "        X.append(queue[0]['postag'])\n",
    "        X.append(queue[0]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    try:\n",
    "        X.append(queue[1]['postag'])\n",
    "        X.append(queue[1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X2 = X\n",
    "    try:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i]['form'] == stack[0]['form']:\n",
    "                X.append(sentence[i+1]['postag'])\n",
    "                X.append(sentence[i+1]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "        \n",
    "    try:\n",
    "        X.append(sentence[int(stack[1]['head'])]['postag'])\n",
    "        X.append(sentence[int(stack[1]['head'])]['word'])\n",
    "        #X.append(queue[2]['form'])\n",
    "    except:\n",
    "        X.append(\"nil\")\n",
    "        X.append(\"nil\")\n",
    "    X3 = X\n",
    "    X1 = dict(zip(feature_names[:6], X1))\n",
    "    X2 = dict(zip(feature_names[:10], X2))\n",
    "    X3 = dict(zip(feature_names, X3))\n",
    "    return X1, X2, X3\n",
    "\n",
    "def encode_classes(y_symbols):\n",
    "    \n",
    "    # We extract the chunk names\n",
    "    classes = sorted(list(set(y_symbols)))\n",
    "    \n",
    "    # We assign each name a number\n",
    "    dict_classes = dict(enumerate(classes))\n",
    "\n",
    "    # We build an inverted dictionary\n",
    "    inv_dict_classes = {v: k for k, v in dict_classes.items()}\n",
    "\n",
    "    # We convert y_symbols into a numerical vector\n",
    "    y = [inv_dict_classes[i] for i in y_symbols]\n",
    "    \n",
    "    return y, dict_classes, inv_dict_classes  \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_file = './swedish_talbanken05_train.conll'\n",
    "    column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "    feature_names = ['can-la', 'can-re','stack0_POS', 'stack0_word', 'stack1_POS', 'stack1_word', 'queue0_POS', 'queue0_word', 'queue1_POS', 'queue1_word', 'stack0fw_POS', 'stack0fw_word','stack1h_POS','queue2_word']\n",
    "\n",
    "    train_sentences = conll.read_sentences(train_file)\n",
    "    train_formatted_corpus = conll.split_rows(train_sentences, column_names_2006)\n",
    "    \n",
    "    X1_dict = []\n",
    "    X2_dict = []\n",
    "    X3_dict = []\n",
    "    Y = []\n",
    "    print(\"Extracting the features...\")\n",
    "    for sentence in train_formatted_corpus:\n",
    "        stack = []\n",
    "        queue = list(sentence)\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        while queue:\n",
    "            x1, x2, x3 = extract(stack, queue, graph, feature_names, sentence)\n",
    "            X1_dict.append(x1)\n",
    "            X2_dict.append(x2)\n",
    "            X3_dict.append(x3)\n",
    "            stack, queue, graph, trans = reference(stack, queue, graph)\n",
    "            Y.append(trans)\n",
    "        stack, graph = transition.empty_stack(stack, graph)\n",
    "\n",
    "        # Poorman's projectivization to have well-formed graphs.\n",
    "        for word in sentence:\n",
    "            word['head'] = graph['heads'][word['id']]\n",
    "        \n",
    "    print(\"Encoding the features and classes...\")\n",
    "    # Vectorize the feature matrix and carry out a one-hot encoding\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    X1 = vec.fit_transform(X1_dict)\n",
    "    X2 = vec.fit_transform(X2_dict)\n",
    "    X3 = vec.fit_transform(X3_dict)\n",
    "    y, dict_classes, inv_dict_classes = encode_classes(Y)\n",
    "        \n",
    "    print(\"Training the models...\")\n",
    "    classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "    \n",
    "    print(\"Training the first model...\")\n",
    "    model1_train_time = time.clock() \n",
    "    #model1 = classifier.fit(X1, y)\n",
    "    print(\"Model 1 training time = \" + str((time.clock() - model1_train_time)/60))\n",
    "    \n",
    "    print(\"Training the second model...\")\n",
    "    model2_train_time = time.clock()\n",
    "    #model2 = classifier.fit(X2, y)\n",
    "    print(\"Model 2 training time = \" + str((time.clock() - model2_train_time)/60))\n",
    "    \n",
    "    print(\"Training the third model...\")\n",
    "    model3_train_time = time.clock()\n",
    "    model3 = classifier.fit(X3, y)\n",
    "    print(\"Model 3 training time = \" + str((time.clock() - model2_train_time)/60))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the test set...\n",
      "Test time: 0.03655246666666396\n"
     ]
    }
   ],
   "source": [
    "def parse_ml(stack, queue, graph, trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    if stack and trans[:2] == 'la':\n",
    "        stack, queue, graph = transition.left_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'la'\n",
    "    if stack and trans == 're':\n",
    "        stack, queue, graph = transition.reduce(stack, queue, graph)\n",
    "        return stack, queue, graph, 're'\n",
    "    stack, queue, graph = transition.shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'\n",
    "\n",
    "test_file = './swedish_talbanken05_test_blind.conll'\n",
    "column_names_2006_test = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats']\n",
    "\n",
    "test_sentences = conll.read_sentences(test_file)\n",
    "test_formatted_corpus = conll.split_rows(test_sentences, column_names_2006_test)\n",
    "\n",
    "y_test_predicted_symbols = []\n",
    "test_start_time = time.clock()\n",
    "f_out = open('out', 'w') \n",
    "print(\"Predicting the test set...\")\n",
    "for sentence in test_formatted_corpus:\n",
    "    stack = []\n",
    "    queue = list(sentence)\n",
    "    graph = {}\n",
    "    graph['heads'] = {}\n",
    "    graph['heads']['0'] = '0'\n",
    "    graph['deprels'] = {}\n",
    "    graph['deprels']['0'] = 'ROOT'\n",
    "    while queue:\n",
    "        x1, x2, x3 = extract(stack, queue, graph, feature_names, sentence)\n",
    "        x_test = vec.transform(x3)\n",
    "        trans = dict_classes[classifier.predict(x_test)[0]]\n",
    "        stack, queue, graph, trans = parse_ml(stack, queue, graph, trans)\n",
    "    stack, graph = transition.empty_stack(stack, graph)\n",
    "\n",
    "    # Poorman's projectivization to have well-formed graphs.\n",
    "    for word in sentence:\n",
    "        word['head'] = graph['heads'][word['id']]\n",
    "        word['deprels'] = graph['deprels'][word['id']]\n",
    "    \n",
    "    \n",
    "    sentence = [list(sentence[i].values()) for i in range(1, len(sentence))]\n",
    "    rows = []\n",
    "    for i in range(len(sentence)):\n",
    "        row = ''\n",
    "        for j in range(len(sentence[i])):\n",
    "            row += sentence[i][j]\n",
    "            row += ' '\n",
    "        rows.append(row)\n",
    "        \n",
    "    for row in rows:\n",
    "        f_out.write(row + '\\n')\n",
    "    f_out.write('\\n')\n",
    "f_out.close()\n",
    "print(\"Test time:\", (time.clock() - test_start_time) / 60)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
